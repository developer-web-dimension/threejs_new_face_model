<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MediaPipe Blendshapes Face Tracking</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body {
      margin: 0;
      background-color: black;
      overflow: hidden;
      width: 100vw;
      height: 100vh;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    
    .container {
      position: relative;
      width: 100%;
      height: 100%;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    
    canvas {
      position: absolute;
      transform: scaleX(-1);
      z-index: 1;
      max-width: 100vw;
      max-height: 100vh;
      object-fit: contain;
    }
    
    video {
      display: none;
    }

    /* Mobile optimizations */
    @media screen and (max-width: 768px) {
      canvas {
        width: 100vw !important;
        height: 100vh !important;
        object-fit: cover;
      }
    }

    @media screen and (orientation: portrait) {
      canvas {
        object-fit: cover;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <video autoplay playsinline id="video"></video>
  </div>

  <script async src="https://unpkg.com/es-module-shims@1.3.6/dist/es-module-shims.js"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.skypack.dev/three@0.150.1",
        "three/": "https://cdn.skypack.dev/three@0.150.1/"
      }
    }
  </script>

  <script type="module">
    import * as THREE from "three";
    import { OrbitControls } from "three/examples/jsm/controls/OrbitControls";
    import { GLTFLoader } from "three/examples/jsm/loaders/GLTFLoader";
    import {
      FilesetResolver,
      FaceLandmarker
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.1.0-alpha-16";

    function getViewportSizeAtDepth(camera, depth) {
      const viewportHeightAtDepth =
        2 * depth * Math.tan(THREE.MathUtils.degToRad(0.5 * camera.fov));
      const viewportWidthAtDepth = viewportHeightAtDepth * camera.aspect;
      return new THREE.Vector2(viewportWidthAtDepth, viewportHeightAtDepth);
    }

    function createCameraPlaneMesh(camera, depth, material) {
      if (camera.near > depth || depth > camera.far) {
        console.warn("Camera plane geometry will be clipped by the `camera`!");
      }
      const viewportSize = getViewportSizeAtDepth(camera, depth);
      const cameraPlaneGeometry = new THREE.PlaneGeometry(
        viewportSize.width,
        viewportSize.height
      );
      cameraPlaneGeometry.translate(0, 0, -depth);

      return new THREE.Mesh(cameraPlaneGeometry, material);
    }

    class BasicScene {
      constructor() {
        this.lastTime = 0;
        this.callbacks = [];
        
        // Get responsive dimensions
        this.updateDimensions();
        
        // Set up the Three.js scene, camera, and renderer
        this.scene = new THREE.Scene();
        this.camera = new THREE.PerspectiveCamera(
          60,
          this.width / this.height,
          0.01,
          5000
        );

        this.renderer = new THREE.WebGLRenderer({ antialias: true });
        this.renderer.setSize(this.width, this.height);
        this.renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));
        THREE.ColorManagement.legacy = false;
        this.renderer.outputColorSpace = THREE.SRGBColorSpace;
        
        // Make canvas responsive
        this.renderer.domElement.style.width = '100%';
        this.renderer.domElement.style.height = '100%';
        document.querySelector('.container').appendChild(this.renderer.domElement);

        // Set up the basic lighting for the scene
        const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
        this.scene.add(ambientLight);
        const directionalLight = new THREE.DirectionalLight(0xffffff, 0.5);
        directionalLight.position.set(0, 1, 0);
        this.scene.add(directionalLight);

        // Set up the camera position and controls
        this.camera.position.z = 0;

        // Add a video background
        const video = document.getElementById("video");
        const inputFrameTexture = new THREE.VideoTexture(video);
        if (!inputFrameTexture) {
          throw new Error("Failed to get the 'input_frame' texture!");
        }
        inputFrameTexture.colorSpace = THREE.SRGBColorSpace;
        const inputFramesDepth = 500;
        const inputFramesPlane = createCameraPlaneMesh(
          this.camera,
          inputFramesDepth,
          new THREE.MeshBasicMaterial({ map: inputFrameTexture })
        );
        this.scene.add(inputFramesPlane);

        // Render the scene
        this.render();

        window.addEventListener("resize", this.resize.bind(this));
        window.addEventListener("orientationchange", () => {
          setTimeout(() => this.resize(), 100);
        });
      }

      updateDimensions() {
        const aspectRatio = 1280 / 720; // Camera aspect ratio
        const windowAspect = window.innerWidth / window.innerHeight;
        
        if (windowAspect > aspectRatio) {
          // Window is wider than camera feed
          this.height = window.innerHeight;
          this.width = this.height * aspectRatio;
        } else {
          // Window is taller than camera feed
          this.width = window.innerWidth;
          this.height = this.width / aspectRatio;
        }
        
        // On mobile, use full screen
        if (window.innerWidth <= 768) {
          this.width = window.innerWidth;
          this.height = window.innerHeight;
        }
      }

      resize() {
        this.updateDimensions();
        
        this.camera.aspect = this.width / this.height;
        this.camera.updateProjectionMatrix();

        this.renderer.setSize(this.width, this.height);
        this.renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));

        // Update canvas style for responsiveness
        this.renderer.domElement.style.width = '100%';
        this.renderer.domElement.style.height = '100%';

        this.renderer.render(this.scene, this.camera);
      }

      render(time = this.lastTime) {
        const delta = (time - this.lastTime) / 1000;
        this.lastTime = time;
        // Call all registered callbacks with deltaTime parameter
        for (const callback of this.callbacks) {
          callback(delta);
        }
        // Render the scene
        this.renderer.render(this.scene, this.camera);
        // Request next frame
        requestAnimationFrame((t) => this.render(t));
      }
    }

    class Avatar {
      constructor(url, scene) {
        this.loader = new GLTFLoader();
        this.morphTargetMeshes = [];
        this.url = url;
        this.scene = scene;
        this.loadModel(this.url);
      }

      loadModel(url) {
        this.url = url;
        this.loader.load(
          // URL of the model you want to load
          url,
          // Callback when the resource is loaded
          (gltf) => {
            if (this.gltf) {
              // Reset GLTF and morphTargetMeshes if a previous model was loaded.
              this.gltf.scene.remove();
              this.morphTargetMeshes = [];
            }
            this.gltf = gltf;
            console.log();
            this.scene.add(gltf.scene);
            this.init(gltf);
          },

          // Called while loading is progressing
          (progress) =>
            console.log(
              "Loading model...",
              100.0 * (progress.loaded / progress.total),
              "%"
            ),
          // Called when loading has errors
          (error) => console.error(error)
        );
      }

      init(gltf) {
        gltf.scene.traverse((object) => {
          // Register first bone found as the root
          if (object.isBone && !this.root) {
            this.root = object;
            console.log(object);
          }
          // Return early if no mesh is found.
          if (!object.isMesh) {
            return;
          }

          const mesh = object;
          // Reduce clipping when model is close to camera.
          mesh.frustumCulled = false;

          // Return early if mesh doesn't include morphable targets
          if (!mesh.morphTargetDictionary || !mesh.morphTargetInfluences) {
            return;
          }
          this.morphTargetMeshes.push(mesh);
        });
      }

      updateBlendshapes(blendshapes) {
        for (const mesh of this.morphTargetMeshes) {
          if (!mesh.morphTargetDictionary || !mesh.morphTargetInfluences) {
            continue;
          }
          for (const [name, value] of blendshapes) {
            if (!Object.keys(mesh.morphTargetDictionary).includes(name)) {
              continue;
            }

            const idx = mesh.morphTargetDictionary[name];
            mesh.morphTargetInfluences[idx] = value;
          }
        }
      }

      applyMatrix(matrix, matrixRetargetOptions) {
        const { decompose = false, scale = 1 } = matrixRetargetOptions || {};
        if (!this.gltf) {
          return;
        }

        matrix.scale(new THREE.Vector3(scale, scale, scale));
        this.gltf.scene.matrixAutoUpdate = false;
        // Set new position and rotation from matrix
        this.gltf.scene.matrix.copy(matrix);
      }

      offsetRoot(offset, rotation) {
        if (this.root) {
          this.root.position.copy(offset);
          if (rotation) {
            let offsetQuat = new THREE.Quaternion().setFromEuler(
              new THREE.Euler(rotation.x, rotation.y, rotation.z)
            );
            this.root.quaternion.copy(offsetQuat);
          }
        }
      }
    }

    let faceLandmarker;
    let video;

    const scene = new BasicScene();
    // Change this path to your local model file
    const avatar = new Avatar(
      "https://assets.codepen.io/9177687/raccoon_head.glb",
      scene.scene
    );

    function detectFaceLandmarks(time) {
      if (!faceLandmarker) {
        return;
      }
      const landmarks = faceLandmarker.detectForVideo(video, time);

      // Apply transformation
      const transformationMatrices = landmarks.facialTransformationMatrixes;
      if (transformationMatrices && transformationMatrices.length > 0) {
        let matrix = new THREE.Matrix4().fromArray(transformationMatrices[0].data);
        // Example of applying matrix directly to the avatar
        avatar.applyMatrix(matrix, { scale: 40 });
      }

      // Apply Blendshapes
      const blendshapes = landmarks.faceBlendshapes;
      if (blendshapes && blendshapes.length > 0) {
        const coefsMap = retarget(blendshapes);
        avatar.updateBlendshapes(coefsMap);
      }
    }

    function retarget(blendshapes) {
      const categories = blendshapes[0].categories;
      let coefsMap = new Map();
      for (let i = 0; i < categories.length; ++i) {
        const blendshape = categories[i];
        // Adjust certain blendshape values to be less prominent.
        switch (blendshape.categoryName) {
          case "browOuterUpLeft":
            blendshape.score *= 1.2;
            break;
          case "browOuterUpRight":
            blendshape.score *= 1.2;
            break;
          case "eyeBlinkLeft":
            blendshape.score *= 1.2;
            break;
          case "eyeBlinkRight":
            blendshape.score *= 1.2;
            break;
          default:
        }
        coefsMap.set(categories[i].categoryName, categories[i].score);
      }
      return coefsMap;
    }

    function onVideoFrame(time) {
      // Do something with the frame.
      detectFaceLandmarks(time);
      // Re-register the callback to be notified about the next frame.
      video.requestVideoFrameCallback(onVideoFrame);
    }

    // Stream webcam into landmarker loop (and also make video visible)
    async function streamWebcamThroughFaceLandmarker() {
      video = document.getElementById("video");

      function onAcquiredUserMedia(stream) {
        video.srcObject = stream;
        video.onloadedmetadata = () => {
          video.play();
        };
      }

      try {
        // Get optimal video constraints based on screen size
        const isMobile = window.innerWidth <= 768;
        const videoConstraints = {
          facingMode: "user",
          width: { ideal: isMobile ? 640 : 1280 },
          height: { ideal: isMobile ? 480 : 720 },
          frameRate: { ideal: 30, max: 60 }
        };

        const evt = await navigator.mediaDevices.getUserMedia({
          audio: false,
          video: videoConstraints
        });
        onAcquiredUserMedia(evt);
        video.requestVideoFrameCallback(onVideoFrame);
      } catch (e) {
        console.error(`Failed to acquire camera feed: ${e}`);
        // Fallback to basic constraints
        try {
          const evt = await navigator.mediaDevices.getUserMedia({
            audio: false,
            video: true
          });
          onAcquiredUserMedia(evt);
          video.requestVideoFrameCallback(onVideoFrame);
        } catch (fallbackError) {
          console.error(`Fallback camera access failed: ${fallbackError}`);
        }
      }
    }

    async function run() {
      await streamWebcamThroughFaceLandmarker();
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.1.0-alpha-16/wasm"
      );
      faceLandmarker = await FaceLandmarker.createFromModelPath(
        vision,
        "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task"
      );
      await faceLandmarker.setOptions({
        baseOptions: {
          delegate: "GPU"
        },
        runningMode: "VIDEO",
        outputFaceBlendshapes: true,
        outputFacialTransformationMatrixes: true
      });

      console.log("Finished Loading MediaPipe Model.");
    }

    run();
  </script>
</body>
</html>